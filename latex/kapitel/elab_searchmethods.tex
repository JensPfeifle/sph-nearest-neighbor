\chapter{Neighbor Search Methods}

The following sections describe tools that can be used to solve the fixed-radius near neighbor search problem described in section~\ref{SECTION:Motivation}. These are implemented in C++ 11 and outfitted with timing code in order to measure their execution times. 

\section{Cell Linked-Lists Method}

The cell linked-lists method (CLL, also cell lists, linked-cell method) divides the calculation domain into cells of edge lengths equal to or greater than the cutoff radius of the interaction search. Therefore, to find the neighbors of a particle, only the cells adjacent to the cell containing the particle must be searched. In the 3D case, the potential neighbors of a particle are found within the 27 cells directly surrounding the particle (\cite{Weygand18}).

The particles themselves are first sorted into two lists, \texttt{first} and \texttt{next}. The list \texttt{first} contains the indices of the first particle in the cell for each cell, i.e. \texttt{first[i]} is the first particle in the  \texttt{i}-th cell.   The list \texttt{next} links a particle \texttt{i} to the index of the next particle \texttt{j} in the same cell, i.e. \texttt{next[i] = j}. If there are no further particles in the cell, \texttt{next} contains \texttt{-1}. In this manner, by starting with the first particle and following the links until the next index is \texttt{-1}, all particles in a cell can be visited in an efficient manner.

To create particle interaction lists for the full domain, the search loops through each cell pair. A cell pair is the current cell and itself, or the current cell and an adjacent cell. For each particle in the current cell, the distance to each particle in the other cell is calculated. If the distance is smaller than the cutoff radius, the interaction is added to the interaction pair lists. For increased efficiency, there is no need to check ``backwards'' into previous adjacent cells, as any interactions between the current cell and the previous cell have already been found.

\section{The {\itshape ANN} Library}
\label{SECTION:ANN}

The {\itshape ANN} library (\cite{ANN10}) implements k-nearest neighbor search using methods based on orthogonal space decomposition. The particles in the search domain are spatially sorted into special data structures. {\itshape ANN} supports kd-trees and box-decomposition trees (bd-trees). The bd-tree includes more decomposition methods than the kd-tree and is more robust for highly clustered data sets. 

{\itshape ANN} supports exact and approximate nearest neighbor searching and a number of distance metrics. In this case, the {\itshape ANN} library is used to perform exact fixed-radius NN searching using the Euclidean ($L_2$) distance metric. Exact searches can be performed by setting the tolerance $\epsilon$ to 0. For fixed-radius searching, {\itshape ANN} provides a procedure \texttt{annkFRSearch}, which returns the $k$ closest points that lie within the radius bound. Because {\itshape ANN} statically allocates its arrays, the \texttt{annkFRSearch} function must be called twice to find all points within the radius: once to find the number of points within the radius, and a second time to actually find the points.

The procedure as implemented in this work is therefore as follows:  First, the search tree structure is initialized using the all data points (particles) in the search domain. Then the \textt{annkFRSearch} procedure is called twice for each data point. Initially, a search is performed using \texttt{annkFRSearch} with $k = 0$ to find the number of points $k'$ that lie within the radius bound.  Then, the appropriate arrays of size $k'$ are allocated, and filled by calling the procedure a second time with $k = k'$. At this point, the neighbors and therefore the interactions of the current point are known. These must then be inserted into the interaction lists, making sure not to duplicate any interactions.

If there is a known upper bound on the possible number of points within the search radius, it would be possible to pre-allocate arrays that are as large as the upper bound, and therefore need to perform the search only once. It is assumed that this would come at the cost of higher memory use, but it was not tested in the scope of this work. For more detailed information about the {\itshape ANN} library, see the {\itshape ANN Programming Manual} (\cite{Mount10}).

\section{The {\itshape nanoflann} Library}
\label{SECTION:NanoFLANN}
{\itshape nanoflann} is a header-only library for C++11 for kd-Tree structures.  It does not support approximate nearest neighbors search.  It does however include a number of optimizations for increased efficiency.  For instance, there are no provisions for choosing between or adding custom NN-search algorithms. Additionally, by using STL containers for the output data, there is no need to call the fixed-radius search method twice, as it is the case with  {\itshape ANN} (\cite{blanco2014nanoflann}).

These optimizations could lead to performance advantages of {\itshape nanoflann} over {\itshape ANN}. It is also able to work with dynamic point clouds without rebuilding the entire kd-tree index. For these reasons, it could be promising for the final application and is mentioned here and included in the source files. However, a comparison to the {\itshape ANN} library is not included in this report.
\end
